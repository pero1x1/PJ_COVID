````markdown
# Анализ метаданных рентгеновских снимков COVID-19 (PySpark)

Учебный проект по дисциплине **«Инфраструктура Big Data»**.  
Проект основан на метаданных датасета **COVID-19 Chest X-Ray Dataset**  
(в репозитории используется локальная копия файла `metadata.csv` и папка `images` со снимками).

---

## Структура репозитория

```text
PJ_COVID/
├── homework.ipynb          # основной Jupyter Notebook с решением
├── metadata.csv            # метаданные по снимкам
├── images/                 # сами изображения (рентген грудной клетки) *
├── processed_parquet_pd/   # сохранённый отфильтрованный датасет в формате Parquet
└── scr/                    # скриншоты SQL-запросов и графиков
    ├── q1.png … q5.png     # результаты 5 SQL-запросов
    └── d1.png … d4.png     # 4 визуализации
````

* Папка `images/` используется только как источник файлов для датасета, в самом ноутбуке мы работаем с метаданными.

---

## Как запустить

Минимальный набор шагов, чтобы воспроизвести ноутбук локально:

1. Клонировать репозиторий:

   ```bash
   git clone https://github.com/pero1x1/PJ_COVID.git
   cd PJ_COVID
   ```

2. Создать окружение (по желанию — `conda` или `venv`) и установить зависимости:

   ```bash
   pip install pyspark pandas matplotlib seaborn pyarrow
   ```

3. Запустить Jupyter Notebook и открыть файл `homework.ipynb`:

   ```bash
   jupyter notebook
   ```

4. В ноутбуке ячейки нужно выполнять сверху вниз.
   При первом запуске PySpark может пару минут подтягивать зависимости.

> **Примечание по Windows.**
> Чистая запись Parquet через `df.write.parquet()` требует настройки `HADOOP_HOME` и `winutils.exe`.
> В ноутбуке для упрощения сохранение в Parquet сделано через `pandas.to_parquet()` (движок `pyarrow`) — формат от этого остаётся тем же.

---

## Что сделано в ноутбуке

### 1. Подготовка среды

* Установка и импорт `pyspark`, `pandas`, `matplotlib`, `seaborn`.
* Создание `SparkSession` с профилем `local[*]` и проверка версии Spark.
* Настройка парсинга дат (политика `spark.sql.legacy.timeParserPolicy`), чтобы аккуратно разобрать строки формата `January 22, 2020` и `Oct 8, 2010`.

### 2. Загрузка и первичный осмотр данных

* Чтение `metadata.csv` в Spark DataFrame.
* Просмотр схемы: поля пациента, пола, возраста, диагноза, проекции (`view`), дат исследования и клинических примечаний.
* Удаление служебного столбца `_c29` и проверка дубликатов (их не оказалось — 950 строк до и после `dropDuplicates`).

### 3. Предобработка и анализ качества данных

* Подсчёт пропусков по всем колонкам.
  Отдельно видно, что у клинических показателей (`leukocyte_count`, `temperature`, `pO2_saturation` и др.) пропусков 90%+ — они мало подходят для аналитики.
* Заполнение ключевых полей:

  * `age` — медианой (54 года);
  * `sex`, `view`, `finding` — заполнение/нормализация строк.
* Проверка, что в базовых колонках (`age`, `sex`, `view`, `finding`) больше нет пропусков.

### 4. Пользовательские функции (унификация диагнозов и возрастные группы)

По смыслу здесь используются UDF, но из-за ограничений PySpark на Windows логика реализована через выражения `when/otherwise`:

* `finding_unified` — свёртка исходных диагноза в 4 группы:

  * `COVID-19`;
  * `Pneumonia (non-COVID)` — все пневмонии без явного указания COVID-19;
  * `Normal`;
  * `Other` / `Unknown`.
* `age_group` — возрастные группы:

  * `18–39`, `40–59`, `60+` (детей в данных практически нет, поэтому группа `0–17` почти не используется).

---

## SQL-аналитика (5 запросов)

Все запросы выполняются через `spark.sql` по временной таблице `covid_data`.

### Q1 — Базовая статистика по диагнозам (`scr/q1.png`)

```sql
SELECT
    finding_unified AS diagnosis,
    COUNT(*) AS cnt,
    ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS pct,
    ROUND(AVG(age), 1) AS avg_age
FROM covid_data
GROUP BY finding_unified
ORDER BY cnt DESC;
```

Основные числа:

* **COVID-19** — 584 записей (~61%);
* **Pneumonia (non-COVID)** — 242 записей (~25%);
* **Normal** — 22 записи (~2%);
* остальное — `Unknown` и `Other`.

Средний возраст пациентов с COVID-19 — около 56 лет.

### Q2 — Распределение по полу и диагнозам (`scr/q2.png`)

Агрегация по `sex` и `finding_unified`:

* Среди мужчин COVID-19 зафиксирован 346 раз, среди женщин — 175 раз.
* Есть небольшая доля записей с неопределённым полом (`sex = Unknown`), для них тоже считаются частоты по диагнозам.

### Q3 — Оконная функция: топ-3 по возрасту в каждой группе диагнозов (`scr/q3.png`)

Используется `ROW_NUMBER() OVER (PARTITION BY finding_unified ORDER BY age DESC)`:

* Для каждой группы диагнозов выбирается трое самых возрастных пациентов.
* Можно увидеть, что в группе COVID-19 встречаются пациенты 90+ лет.

### Q4 — Временные тренды по датам исследований (`scr/q4.png`)

Агрегация по месяцам `year_month` и диагнозам:

* До 2019 года в данных почти только пневмонии.
* В **декабре 2019 — начале 2020** появляется резкий всплеск количества снимков с диагнозом `COVID-19`.

### Q5 — Диагнозы по проекциям снимков (`scr/q5.png`)

Статистика по `view`:

* Больше всего снимков в проекциях **PA** и **AP**.
* Для каждой проекции считается количество и доля диагнозов (`pct_within_view`).
* Например, для PA:

  * ~57% снимков — COVID-19,
  * ~34% — пневмония,
  * остальное — нормальные/прочие случаи.

---

## Визуализации

Скрипты построения графиков находятся в `homework.ipynb`, итоговые изображения — в папке `scr`.

### D1 — Круговая диаграмма диагнозов (`scr/d1.png`)

* Примерно **66%** — COVID-19,
* **~32%** — пневмония без COVID,
* **~2%** — нормальные снимки.

### D2 — Возрастные группы (`scr/d2.png`)

* Больше всего пациентов в группе **40–59 лет**.
* На втором месте — **60+**.
* Группа 18–39 заметно меньше — COVID-19 и тяжёлые пневмонии в этом датасете чаще встречаются у более старших пациентов.

### D3 — Временные тренды (`scr/d3.png`)

* До 2020 года кривая пневмоний идёт на небольшом уровне.
* В 2020-м на графике появляется резкий пик по COVID-19 — видно начало пандемии.
* Нормальные снимки присутствуют, но в гораздо меньшем количестве.

### D4 — Heatmap по проекциям (`scr/d4.png`)

* Для каждой проекции (`PA`, `AP`, `AP Supine`, `L`, `AP Erect`, `Axial`, `Coronal`) показано, сколько в ней случаев каждого диагноза.
* Больше всего COVID-19 в проекциях **PA** и **AP Supine**, что логично для тяжёлых пациентов (лежачие).

---

## Сохранение в Parquet

После фильтрации формируется поднабор:

* только `modality = 'X-ray'`;
* адекватные возраста (0–120);
* не пустые `view` и осмысленные `finding_unified`.

Из-за ограничений Hadoop на Windows запись сделана через:

1. преобразование `filtered_df` в pandas;
2. сохранение в `processed_parquet_pd/covid_filtered.parquet` с помощью `to_parquet()` (движок `pyarrow`).

---

## Основные выводы

Коротко по результатам:

* В доступных данных доминируют пациенты с **COVID-19** и **пневмонией**, нормальных снимков мало — датасет явно «смещён» в сторону патологии.
* Больше всего случаев приходится на возраст **40+**, и особенно на группу 40–59 лет.
* По времени видно, как COVID-19 появляется в статистике только в конце 2019 года и сразу даёт резкий всплеск.
* Распределение диагнозов по проекциям показывает, что основная нагрузка идёт на стандартные проекции PA/AP и «лежачие» AP Supine.

Проект учебный, но вполне показывает, как с помощью PySpark можно:

* загрузить и почистить реальные медицинские данные,
* сделать SQL-аналитику с оконными функциями,
* подготовить набор для дальнейшего анализа/моделирования.

```
```
